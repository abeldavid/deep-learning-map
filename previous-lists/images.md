# Images

(Initialised 11 June 2018, is very new!)

- [Self-Attention Generative Adversarial Network (SAGAN) (Zhang et. al., May 2018)](https://arxiv.org/abs/1805.08318)
	- "The self-attention module is complementary to convolutions and helps with modeling long range, multi-level dependencies across image regions. Armed with self-attention, the generator can draw images in which fine details at every location are carefully coordinated with fine details in distant portions of the image"
	- SotA by far when measured by Inception score (KL)
	- 